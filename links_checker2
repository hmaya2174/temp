import urllib.request
from urllib.error import URLError
import re
from datetime import datetime

def read_urls(filename):
    with open(filename, 'r') as file:
        urls = file.readlines()
    if len(urls) > 2:
        raise ValueError("Maximum of 2 URLs only")
    return [url.strip() for url in urls]

def scan_links(url):
    try:
        response = urllib.request.urlopen(url)
        data = response.read().decode('utf-8')
        links = re.findall(r'href="(http[s]?://.*?)"', data)
        return links
    except URLError:
        return []

def check_link(url):
    if 'myintranet' in url:
        url = url.replace('myintranet', 'intranet')
    try:
        urllib.request.urlopen(url)
        return "URL Working"
    except URLError:
        return "Broken"

def main():
    try:
        urls = read_urls("weblist.txt")
        output_filename = "brokenLinksReport" + datetime.now().strftime("%Y%m%d%H%M%S") + ".txt"
        with open(output_filename, 'w') as output_file:
            for index, url in enumerate(urls, start=1):
                links = scan_links(url)
                for link in links:
                    status = check_link(link)
                    output_file.write(f"{index}|{link}|{status}\n")
        print(f"Report generated: {output_filename}")
    except ValueError as e:
        print(str(e))

if __name__ == "__main__":
    main()

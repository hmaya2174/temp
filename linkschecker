import urllib.request
import re
import datetime

def find_links(html_content):
    """Extracts all links from the given HTML content."""
    link_pattern = re.compile(r'href=["\'](http[s]?://[^"\'\s]+)["\']')
    return link_pattern.findall(html_content)

def check_url(url):
    """Check if the URL is working or broken."""
    try:
        with urllib.request.urlopen(url) as response:
            return "URL Working" if response.status == 200 else "Broken"
    except:
        return "Broken"

def process_webpages(file_name):
    with open(file_name, 'r') as file:
        urls = file.readlines()

    if len(urls) > 2:
        print("Maximum of 2 URLs only")
        return

    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    output_file_name = f"brokenLinksReport{timestamp}.txt"
    with open(output_file_name, 'w') as output_file:
        output_file.write("Order|URL|Status\n")

        for index, url in enumerate(urls):
            url = url.strip()
            try:
                with urllib.request.urlopen(url) as response:
                    html_content = response.read().decode('utf-8')
                    links = find_links(html_content)
                    for link in links:
                        status = check_url(link)
                        output_file.write(f"{index + 1}|{link}|{status}\n")
            except Exception as e:
                output_file.write(f"{index + 1}|{url}|Broken due to {str(e)}\n")

    print(f"Report generated: {output_file_name}")

# Make sure 'weblist.txt' exists in the same directory as this script.
process_webpages('weblist.txt')

import urllib.request
import re
from datetime import datetime

def read_urls(filename):
    with open(filename, 'r') as file:
        urls = [line.strip() for line in file]
    return urls

def fetch_html(url):
    try:
        with urllib.request.urlopen(url) as response:
            return response.read().decode('utf-8')
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return ""

def find_links(html):
    pattern = re.compile(r'<a\s+(?:[^>]*?\s+)?href="([^"]*)"[^>]*>(.*?)</a>', re.IGNORECASE)
    return pattern.findall(html)

def check_url(url):
    try:
        with urllib.request.urlopen(url) as response:
            if response.status == 200:
                return "URL Working"
            else:
                return "Broken"
    except:
        return "Broken"

def generate_report(urls, report_filename):
    with open(report_filename, 'w', encoding='utf-8') as report_file:
        report_file.write("Order|Anchor URL|Link Title|Status\n")
        for i, url in enumerate(urls, 1):
            html = fetch_html(url)
            links = find_links(html)
            for link in links:
                link_url, link_title = link
                if 'myintranet' in link_url:
                    link_url = link_url.replace('myintranet', 'intranet')
                status = check_url(link_url)
                report_file.write(f"{i}|{link_url}|{link_title.strip()}|{status}\n")

def main():
    filename = "weblist.txt"
    urls = read_urls(filename)

    if len(urls) > 2:
        print("Maximum of 2 URLs only")
        return

    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    report_filename = f"gatherlinksreport_{timestamp}.txt"
    generate_report(urls, report_filename)
    print(f"Report generated: {report_filename}")

if __name__ == "__main__":
    main()
